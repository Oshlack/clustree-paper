---
title: "Clustering trees, a visualisation for examining clusterings a increasing resolutions"
author:
- Luke Zappia (1, 2)
- Alicia Oshlack (1, 2)
date: "1 Bioinformatics, Murdoch Children's Research Institute; 2 School of Biosciences, University of Melbourne"
output:
    bookdown::word_document2:
        reference_docx: style/style.docx
        pandoc_args: [ "--csl", "style/nature.csl" ]
bibliography: style/references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = TRUE,
                      cache.path     = "cache/",
                      cache.comments = TRUE,
                      echo           = FALSE,
                      error          = FALSE,
                      fig.path       = "figures/",
                      fig.width      = 10,
                      fig.height     = 8,
                      dev            = c('png', 'pdf'),
                      message        = FALSE,
                      warning        = FALSE)
```

```{r libaries}
library("clustree")
library("Seurat")
library("Matrix")

library("here")
```

```{r source}

```

```{r palette}

```

```{r load}

```

# Abstract

# Keywords

# Introduction

* Clustering used for many things
* Selecting number of clusters is hard
* Usually measure a single clustering
* Look at changes across resolutions

Clustering analysis is commonly used to group similar samples for a diverse
range of applications. Typically the goal of clustering is to form groups where
each sample is more similar to the other samples in the same group than to
samples in other groups. While other forms of clustering exist, such as fuzzy or
soft clustering where each sample is assigned to every cluster with some
probability or hierarchical clustering which forms a tree of samples, most
methods form hard clusters where each sample is assigned to a single group.
This goal can be achieved in a variety of ways, for example by considering the
distances between sample, areas of density across the dataset or relationships
to statistical distributions.

In many cases the number of groups that should be present in a dataset is not
known in advance and deciding the correct number of clusters to use is a
significant challenge. For some algorithms, such as $k$-means clustering, the
number of clusters must be explicitly provided while others have parameters
that, directly or indirectly, control the resolution of the clustering produced.
While there are methods and statistics (such as the elbow method or silhouette)
designed to help analysts decide which clustering resolution to use they
typically only consider a single set of samples or clusters at a time.

An alternative approach would be to consider clusterings at multiple resolutions
and look at how samples change groupings as the number of clusters increases.
This is the approach taken by the clustering tree visualisation we present here:
a dataset is clustered at multiple resolutions, the overlap between clusters at
adjacent resolutions is used to build edges and the resulting graph is
presented as a tree. This tree can be used to examine how clusters are
related to each other, which clusters are distinct and which are unstable. In
the following sections we describe how we construct such a tree and present
examples of trees built from a classical clustering dataset and a complex
single-cell RNA-sequencing (scRNA-seq) dataset.

# Building a clustering tree

* Cluster data at multiple resolutions
* Compare clusterings at adjacent resolutions
* Calculate statistics
* Build graph
* Visualise tree

# A simple example

* Iris dataset
* Clustered using k-means
* Features of tree...

```{r iris-tree}
data("iris_clusts")

clustree(iris_clusts, prefix = "K")
```

# Clustering trees for scRNA-seq data

* Clustering used for scRNA-seq data
* PBMC dataset
* Clustered using Seurat
* Features of tree...

```{r seurat-load}
pbmc_data <- Read10X(here("data"))
pbmc <- CreateSeuratObject(raw.data = pbmc_data, min.cells = 3, min.genes = 200,
                           project = "10X_PBMC")
```

```{r seurat-QC}
mito_genes <- grep(pattern = "^MT-", x = rownames(x = pbmc@data), value = TRUE)
percent_mito <- Matrix::colSums(pbmc@raw.data[mito_genes, ]) /
                Matrix::colSums(pbmc@raw.data)
pbmc <- AddMetaData(object = pbmc, metadata = percent_mito,
                    col.name = "percent_mito")
pbmc <- FilterCells(object = pbmc, subset.names = c("nGene", "percent_mito"),
                    low.thresholds = c(200, -Inf),
                    high.thresholds = c(2500, 0.05))
```

```{r seurat-norm}
pbmc <- NormalizeData(object = pbmc, normalization.method = "LogNormalize",
                      scale.factor = 10000, display.progress = FALSE)
pbmc <- FindVariableGenes(object = pbmc, mean.function = ExpMean,
                          dispersion.function = LogVMR, x.low.cutoff = 0.0125,
                          x.high.cutoff = 3, y.cutoff = 0.5, do.plot = FALSE,
                          display.progress = FALSE)
```

```{r seurat-scale}
pbmc <- ScaleData(object = pbmc, vars.to.regress = c("nUMI", "percent_mito"),
                  display.progress = FALSE)
```

```{r seurat-cluster}
pbmc <- RunPCA(object = pbmc, pc.genes = pbmc@var.genes, do.print = FALSE)
pbmc <- FindClusters(object = pbmc, reduction.type = "pca", dims.use = 1:10,
                     resolution = seq(0, 1, 0.1), print.output = FALSE)
```

```{r seurat-tree}
clustree(pbmc)
```

# Discussion and conclusion

* Picking number of clusters is hard
* Clustering trees let you see what happens as resolution increases

# Methods

# Declarations

## Ethics

Not applicable.

## Availability of data and materials

The clustree package is available from our GitHub repository
(https://github.com/Oshlack/clustree) and the code and datasets used for the
analysis in this paper are available from
https://github.com/Oshlack/clustree-paper.

## Competing interests

The authors declare no competing interests.

## Funding

Luke Zappia is supported by an Australian Government Research Training Program
(RTP) Scholarship. Alicia Oshlack is supported through a National Health and
Medical Research Council Career Development Fellowship APP1126157. MCRI is
supported by the Victorian Government's Operational Infrastructure Support
Program.

## Authors' contributions

## Acknowledgements

# Additional files

# References
