---
title: "Clustering trees, a visualisation for examining clusterings a increasing resolutions"
author:
- Luke Zappia (1, 2)
- Alicia Oshlack (1, 2)
date: "1 Bioinformatics, Murdoch Children's Research Institute; 2 School of Biosciences, University of Melbourne"
output:
    bookdown::word_document2:
        reference_docx: style/style.docx
        pandoc_args: [ "--csl", "style/nature.csl" ]
bibliography: style/references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = TRUE,
                      cache.path     = "cache/",
                      cache.comments = TRUE,
                      echo           = FALSE,
                      error          = FALSE,
                      fig.path       = "figures/",
                      fig.width      = 10,
                      fig.height     = 8,
                      dev            = c('png', 'pdf'),
                      message        = FALSE,
                      warning        = FALSE)
```

```{r libaries, cache = FALSE}
library("clustree")
library("Seurat")
library("Matrix")

library("here")
library("knitr")
```

```{r source}

```

```{r palette}

```

```{r load}

```

# Abstract

# Keywords

# Introduction

* Clustering used for many things
* Selecting number of clusters is hard
* Usually measure a single clustering
* Look at changes across resolutions

Clustering analysis is commonly used to group similar samples for a diverse
range of applications. Typically the goal of clustering is to form groups where
each sample is more similar to the other samples in the same group than to
samples in other groups. While other forms of clustering exist, such as fuzzy or
soft clustering where each sample is assigned to every cluster with some
probability or hierarchical clustering which forms a tree of samples, most
methods form hard clusters where each sample is assigned to a single group.
This goal can be achieved in a variety of ways, for example by considering the
distances between sample, areas of density across the dataset or relationships
to statistical distributions.

In many cases the number of groups that should be present in a dataset is not
known in advance and deciding the correct number of clusters to use is a
significant challenge. For some algorithms, such as $k$-means clustering, the
number of clusters must be explicitly provided while others have parameters
that, directly or indirectly, control the resolution of the clustering produced.
While there are methods and statistics (such as the elbow method or silhouette)
designed to help analysts decide which clustering resolution to use they
typically only consider a single set of samples or clusters at a time.

An alternative approach would be to consider clusterings at multiple resolutions
and look at how samples change groupings as the number of clusters increases.
This is the approach taken by the clustering tree visualisation we present here:
a dataset is clustered at multiple resolutions, the overlap between clusters at
adjacent resolutions is used to build edges and the resulting graph is
presented as a tree. This tree can be used to examine how clusters are
related to each other, which clusters are distinct and which are unstable. In
the following sections we describe how we construct such a tree and present
examples of trees built from a classical clustering dataset and a complex
single-cell RNA-sequencing (scRNA-seq) dataset. The kinds of figures shown here
can be produced in R using our publicly available clustree package.

# Building a clustering tree

* Cluster data at multiple resolutions
* Compare clusterings at adjacent resolutions
* Calculate statistics
* Build graph
* Visualise tree

To build a clustering tree we start with a set of clusterings allocating samples
to groups at different resolutions. These could be produced using any
hard-clustering algorithm that allows control of the number of clusters in
some way. For example this could be a set of samples clustered using $k$-means
with $k = 1, 2, 3$ as shown in Figure \@ref(fig:algorithm). We then sort these 
clusterings so that they are ordered by increasing resolution (here that would
be $k$), then consider pairs of adjacent clusterings. Each cluster at a specific 
resolution ($k$) is compared with each cluster in the next highest resolution
($k + 1$) and the overlap is computed as the number of samples that are assigned
to both. We next build a graph where each node is a cluster and each edge is an
overlap between clusters.

```{r algorithm, fig.cap = "Illustration of the steps required to build a clustering tree. First a dataset must be clustered at different resolutions. A graph in then constructed with nodes representing clusters and edges showing the overlap in samples between them. Statistics are then calculate, edges filtered and the final graph visualised.", cache = FALSE}
include_graphics(here("figures/algorithm.png"))
```

Many of the edges will be empty (no samples in Cluster A at resolution $k$ end
up in Cluster B at resolution $k + 1$). In some datasets there may also be edges
that contain few samples which are not informative and result in a cluttered
tree. This is easily handled by setting a threshold number of samples to remove
low-count edges. While this approach can be useful for removing edges from the
graph it is often more informative to instead emphasise more important edges.
In this case the count of samples is not the correct statistic to use as it
naturally favours edges at higher resolutions and those connecting larger
clusters. Instead we propose using the in proportion, the ratio between the
number of samples on the edge and the number of samples in the cluster it goes
towards. This measure shows the importance of the edge to the higher resolution
cluster, independently of the cluster size. We can apply a second threshold to
this value in order to remove less important edges.

We then visualise the final graph using a tree layout. This places the cluster
nodes in a series of layers where each layer is a different resolution and edges
show the transfer of samples through those resolutions. Edges are coloured
according to the number of samples they represent and the in proportion is used
to control the edge transparency, highlighting more important edges. By default
nodes are sized according the size of the cluster they represent, with their
colour indicating the resolution. The clustree package also includes options
for controlling the aesthetics of nodes based on the attributes of samples in
the clusters they represent.

# A simple example

* Iris dataset
* Clustered using k-means
* Features of tree...

```{r iris-tree}
data("iris_clusts")

clustree(iris_clusts, prefix = "K")
```

# Clustering trees for scRNA-seq data

* Clustering used for scRNA-seq data
* PBMC dataset
* Clustered using Seurat
* Features of tree...

```{r seurat-load}
pbmc_data <- Read10X(here("data"))
pbmc <- CreateSeuratObject(raw.data = pbmc_data, min.cells = 3, min.genes = 200,
                           project = "10X_PBMC")
```

```{r seurat-QC}
mito_genes <- grep(pattern = "^MT-", x = rownames(x = pbmc@data), value = TRUE)
percent_mito <- Matrix::colSums(pbmc@raw.data[mito_genes, ]) /
                Matrix::colSums(pbmc@raw.data)
pbmc <- AddMetaData(object = pbmc, metadata = percent_mito,
                    col.name = "percent_mito")
pbmc <- FilterCells(object = pbmc, subset.names = c("nGene", "percent_mito"),
                    low.thresholds = c(200, -Inf),
                    high.thresholds = c(2500, 0.05))
```

```{r seurat-norm}
pbmc <- NormalizeData(object = pbmc, normalization.method = "LogNormalize",
                      scale.factor = 10000, display.progress = FALSE)
pbmc <- FindVariableGenes(object = pbmc, mean.function = ExpMean,
                          dispersion.function = LogVMR, x.low.cutoff = 0.0125,
                          x.high.cutoff = 3, y.cutoff = 0.5, do.plot = FALSE,
                          display.progress = FALSE)
```

```{r seurat-scale}
pbmc <- ScaleData(object = pbmc, vars.to.regress = c("nUMI", "percent_mito"),
                  display.progress = FALSE)
```

```{r seurat-cluster}
pbmc <- RunPCA(object = pbmc, pc.genes = pbmc@var.genes, do.print = FALSE)
pbmc <- FindClusters(object = pbmc, reduction.type = "pca", dims.use = 1:10,
                     resolution = seq(0, 1, 0.1), print.output = FALSE)
```

```{r seurat-tree}
clustree(pbmc)
```

# Discussion and conclusion

* Picking number of clusters is hard
* Clustering trees let you see what happens as resolution increases

# Methods

# Declarations

## Ethics

Not applicable.

## Availability of data and materials

The clustree package is available from our GitHub repository
(https://github.com/Oshlack/clustree) and the code and datasets used for the
analysis in this paper are available from
https://github.com/Oshlack/clustree-paper.

## Competing interests

The authors declare no competing interests.

## Funding

Luke Zappia is supported by an Australian Government Research Training Program
(RTP) Scholarship. Alicia Oshlack is supported through a National Health and
Medical Research Council Career Development Fellowship APP1126157. MCRI is
supported by the Victorian Government's Operational Infrastructure Support
Program.

## Authors' contributions

## Acknowledgements

# Additional files

# References
