---
title: "Clustering trees: a visualisation for examining clusterings a increasing resolutions"
author:
- Luke Zappia (1, 2)
- Alicia Oshlack (1, 2)
date: "1 Bioinformatics, Murdoch Children's Research Institute; 2 School of Biosciences, University of Melbourne"
output:
    bookdown::word_document2:
        reference_docx: style/style.docx
        pandoc_args: [ "--csl", "style/nature.csl" ]
bibliography: style/references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = TRUE,
                      cache.path     = "cache/",
                      cache.comments = TRUE,
                      echo           = FALSE,
                      error          = FALSE,
                      fig.path       = "figures/",
                      fig.width      = 10,
                      fig.height     = 8,
                      dev            = c('png', 'pdf'),
                      message        = FALSE,
                      warning        = FALSE)
```

```{r libaries, cache = FALSE}
# clustree
library("clustree")

# Seurat
library("Seurat")
library("Matrix")

# Plotting
library("ggplot2")
library("cowplot")
library("ggraph")

# Document
library("here")
library("knitr")
```

# Abstract

# Keywords

# Introduction

Clustering analysis is commonly used to group similar samples for a diverse
range of applications. Typically the goal of clustering is to form groups where
each sample is more similar to the other samples in the same group than to
samples in other groups. While other forms of clustering exist, such as fuzzy or
soft clustering where each sample is assigned to every cluster with some
probability, or hierarchical clustering which forms a tree of samples, most
methods form hard clusters where each sample is assigned to a single group. This
goal can be achieved in a variety of ways, for example by considering the
distances between sample ($k$-means [@Macqueen1967-ag], PAM [@Kaufman1990-mt]),
areas of density across the dataset (DBSCAN [@Ester1996-da]) or relationships to
statistical distributions [@Fraley2002-ps].

In many cases the number of groups that should be present in a dataset is not
known in advance and deciding the correct number of clusters to use is a
significant challenge. For some algorithms, such as $k$-means clustering
[@Forgy1965-oj, @Lloyd1982-bd], the number of clusters must be explicitly
provided while other methods have parameters that, directly or indirectly,
control the clustering resolution and therefore the number of clusters produced.
While there are methods and statistics (such as the elbow method
[@Thorndike1953-uv] or silhouette plots [@Rousseeuw1987-sm]) designed to help
analysts decide which clustering resolution to use they typically produce a
single score which only considers a single set of samples or clusters at a time.

An alternative approach would be to consider clusterings at multiple resolutions
and look at how samples change groupings as the number of clusters increases.
This is the approach taken by the clustering tree visualisation we present here:
a dataset is clustered at multiple resolutions, the overlap between clusters at
adjacent resolutions is used to build edges and the resulting graph is
presented as a tree. This tree can be used to examine how clusters are
related to each other, which clusters are distinct and which are unstable. In
the following sections we describe how we construct such a tree and present
examples of trees built from a classical clustering dataset and a complex
single-cell RNA-sequencing (scRNA-seq) dataset. The kinds of figures shown here
can be produced in R using our publicly available clustree package.

# Building a clustering tree

To build a clustering tree we start with a set of clusterings allocating samples
to groups at several different resolutions. These could be produced using any
hard-clustering algorithm that allows control of the number of clusters in some
way. For example this could be a set of samples clustered using $k$-means with
$k = 1, 2, 3$ as shown in Figure \@ref(fig:algorithm). We sort these clusterings
so that they are ordered by increasing resolution ($k$), then consider pairs of
adjacent clusterings. Each cluster $c_{k,i}$ (where $n$ is the number of
clusters at resolution $k$ and $i = 1,...,n$) is compared with each cluster
$c_{k + 1,j}$ (where $m$ is the number of clusters at resolution $k + 1$ and $j =
1,...,m$). The overlap between the two clusters is computed as the number of
samples that are assigned to both $c_{k,i}$ and $c_{k + 1,j}$. We next build a
graph where each node is a cluster and each edge is an overlap between two
clusters.

```{r algorithm, fig.cap = "Illustration of the steps required to build a clustering tree. First a dataset must be clustered at different resolutions. A graph is then constructed with nodes representing clusters and edges showing the overlap in samples between them. Statistics are then calculated, edges filtered and the final graph visualised.", cache = FALSE}
include_graphics(here("figures/algorithm.png"))
```

Many of the edges will be empty, for example in Figure \@ref(fig:algorithm) no
samples in largest cluster at $k = 2$ end up in the smallest cluster at $k = 3$.
In some datasets there may also be edges that contain few samples which are not
informative and result in a cluttered tree. This is easily handled by setting a
threshold number of samples to remove low-count edges. While this approach can
be useful for removing edges from the graph it is often more informative to
instead emphasise more important edges. In this case the count of samples is not
the correct statistic to use as it naturally favours edges at lower resolutions
and those connecting larger clusters. Instead we define the in-proportion metric
as the ratio between the number of samples on the edge and the number of samples
in the cluster it goes towards. This metric shows the importance of the edge to
the higher resolution cluster, independently of the cluster size. We can apply a
second threshold to this value in order to remove less important edges.

We visualise the final graph using a tree layout. This places the cluster nodes
in a series of layers where each layer is a different clustering resolution and
edges show the transition of samples through those resolutions. Edges are
coloured according to the number of samples they represent and the in-proportion
is used to control the edge transparency, highlighting more important edges. By
default, nodes are sized according to the size of the cluster they represent,
with their colour indicating the resolution. The clustree package also includes
options for controlling the aesthetics of nodes based on the attributes of
samples in the clusters they represent.

# A simple example

To further illustrate how a clustering tree is built we will work through an
example using the classical iris dataset [@Anderson1935-mt, @Fisher1936-wt].
This dataset contains measurements of the sepal length, sepal width, petal
length and petal width from 150 iris flowers, 50 from each of three species:
_Iris setosa_, _Iris versicolor_ and _Iris virginica_. The iris dataset is
commonly used as example for both clustering and classification problems with
the _Iris setosa_ samples being significantly different to, and linearly
separable from, the other samples. We have clustered this dataset using
$k$-means clustering with $k = 1,...,5$ and produced the clustering tree shown
in Figure \@ref(fig:iris-tree)A.

```{r iris-tree, fig.cap = "Clustering trees based on $k$-means clustering of the iris dataset. In A nodes are coloured according to the value of $k$ and sized according to the number of samples they represent. Edges are coloured according to the number of samples (from blue representing few to yellow representing many) and the transparency adjusted according to the in-proportion, with stronger lines showing edges that are more important to the higher resolution cluster. Cluster labels are randomly assigned by the $k$-means algorithm. B shows the same tree with the node colouring changed to show the mean petal length of the samples in each cluster."}
data("iris_clusts")

p1 <- clustree(iris_clusts, prefix = "K", node_size_range = c(10, 30),
               node_text_size = 8, edge_width = 4, layout = "sugiyama") +
    guides(size = guide_legend(title = "Cluster size", order = 1),
           colour = guide_legend(title = "k",
                                 keywidth = 5,
                                 override.aes = list(size = 10),
                                 order = 2),
           edge_colour = guide_edge_colourbar(title = "Sample count",
                                              barwidth = 3,
                                              draw.ulim = TRUE,
                                              draw.llim = TRUE,
                                              order = 3),
           edge_alpha = guide_legend(title = "In-proportion",
                                     keywidth = 5,
                                     order = 4)) +
    theme(legend.title = element_text(size = 20),
          legend.text = element_text(size = 15))
p2 <- clustree(iris_clusts, prefix = "K", node_colour = "Petal.Length",
               node_colour_aggr = "mean", node_size_range = c(10, 30),
               node_text_size = 8, edge_width = 4, layout = "sugiyama") +
    scale_colour_viridis_c(option = "plasma", begin = 0.3) +
    guides(size = guide_legend(title = "Cluster size", order = 1),
           colour = guide_colourbar(title = "Petal length",
                                    barwidth = 3,
                                    order = 2),
           edge_colour = guide_edge_colourbar(title = "Sample count",
                                              barwidth = 3,
                                              draw.ulim = TRUE,
                                              draw.llim = TRUE,
                                              order = 3),
           edge_alpha = guide_legend(title = "In-proportion",
                                     keywidth = 5,
                                     order = 4)) +
    theme(legend.title = element_text(size = 20),
          legend.text = element_text(size = 15))
panel <- plot_grid(p1, p2, labels = "AUTO", label_size = 30)

save_plot(here("figures/iris_tree.png"), panel, ncol = 2, base_height = 10)
save_plot(here("figures/iris_tree.pdf"), panel, ncol = 2, base_height = 10)

include_graphics(here("figures/iris_tree.png"))
```

We see that there is one branch of the tree that is clearly distinct (presumably
representing _Iris setosa_), remaining unchanged regardless of the number of
clusters. On the other side we see the cluster at $k = 2$ cleanly split into two
clusters (presumably _Iris versicolor_ and _Iris virginica_) at $k = 3$ but as
we move to $k = 4$ and $k = 5$ we see clusters being formed from multiple
branches with more low proportion edges. This kind of pattern can be an
indication that the data has become over-clustered and we have begun to
introduce artificial groupings. In this case we know that $k = 3$ is the correct
choice but this is also the value that is suggested by this tree.

We can check our assumption that the distinct branch represents the _Iris
setosa_ samples and the other two clusters at $k = 3$ are _Iris versicolor_ and
_Iris virginica_ by overlaying some known information about the samples. In
Figure \@ref(fig:iris-tree)B we have coloured the nodes by the mean petal length
of the samples they contain. We can now see that clusters in the distinct branch
have the shortest petals, with Cluster 1 at $k = 3$ having an intermediate
length and Cluster 3 the longest petals. This feature is known to separate the
samples into the expected species with _Iris setosa_ have the shortest petals on
average, _Iris versicolor_ an intermediate length and _Iris virginica_ the
longest.

Although this is a very simple example it still highlights some of the benefits
of viewing a clustering tree. We get some indication of the correct clustering
resolution by examining the edges and we can overlay known information to assess
the quality of the clustering. For example if we observed that all clusters had
the same mean petal length it would suggest that the clustering has not been
successful as we know this is an important feature known to separate the
species. We could potentially learn more by looking at which samples follow low
proportion edges or overlaying a series of features to try and understand what
particular clusters to split.

# Clustering trees for single-cell RNA-seq data

One field that recently begun to make heavy use of clustering techniques is the
analysis of single-cell RNA-sequencing (scRNA-seq) data. Single-cell
RNA-sequencing is a recently developed technology that can measure how genes are
expressed in thousands to millions of individual cells [@Tang2009-jd]. This
technology has been rapidly adopted in fields like developmental biology and
immunology where it is valuable to have information from single cells rather
than the measurements that are averaged across the many different cells in a
sample using older RNA sequencing technologies. One of the key uses for
scRNA-seq is to discover and interrogate the different cell types present in a
sample of a complex tissue. In this situation clustering is typically used to
group similar cells based on their gene expression and genes that are
differentially expressed between groups are used to infer the identify or
function of those cells [@Stegle2015-mc]. The number of cell types in an
scRNA-seq dataset can vary depending on factors such as the tissue being
studied, it's developmental or environmental state and the number of cells
captured. Often the number of cells types is not known before the data is
generated and some samples can contain dozens of clusters. Therefore deciding
which clustering resolution to use is an important consideration in this
application.

As an example of how clustering trees can be used in the scRNA-seq context we
present an example based on a commonly used Peripheral Blood Mononuclear Cell
(PBMC) dataset. This dataset was originally produced by 10x Genomics and
contains 2700 peripheral blood monocuclear cells, representing a range of
well-studied immune cell types [@Zheng2017-mm]. We have analysed this dataset
using the Seurat package [@Satija2015-or], a commonly used toolkit for scRNA-seq
analysis, following the instructions in their tutorial with the exception of
varying the clustering resolution parameter from zero to one (see methods).
Seurat uses a graph-based clustering algorithm and the resolution parameter
controls the partitioning of this graph, with higher values resulting in more
clusters. The clustering tree produced from this analysis is shown in Figure
\@ref(fig:seurat-tree).

```{r seurat-load}
pbmc_data <- Read10X(here("data"))
pbmc <- CreateSeuratObject(raw.data = pbmc_data, min.cells = 3, min.genes = 200,
                           project = "10X_PBMC")
```

```{r seurat-QC}
mito_genes <- grep(pattern = "^MT-", x = rownames(x = pbmc@data), value = TRUE)
percent_mito <- Matrix::colSums(pbmc@raw.data[mito_genes, ]) /
                Matrix::colSums(pbmc@raw.data)
pbmc <- AddMetaData(object = pbmc, metadata = percent_mito,
                    col.name = "percent_mito")
pbmc <- FilterCells(object = pbmc, subset.names = c("nGene", "percent_mito"),
                    low.thresholds = c(200, -Inf),
                    high.thresholds = c(2500, 0.05))
```

```{r seurat-norm}
pbmc <- NormalizeData(object = pbmc, normalization.method = "LogNormalize",
                      scale.factor = 10000, display.progress = FALSE)
pbmc <- FindVariableGenes(object = pbmc, mean.function = ExpMean,
                          dispersion.function = LogVMR, x.low.cutoff = 0.0125,
                          x.high.cutoff = 3, y.cutoff = 0.5, do.plot = FALSE,
                          display.progress = FALSE)
```

```{r seurat-scale}
pbmc <- ScaleData(object = pbmc, vars.to.regress = c("nUMI", "percent_mito"),
                  display.progress = FALSE)
```

```{r seurat-cluster}
pbmc <- RunPCA(object = pbmc, pc.genes = pbmc@var.genes, do.print = FALSE)
pbmc <- FindClusters(object = pbmc, reduction.type = "pca", dims.use = 1:10,
                     resolution = seq(0, 1, 0.1), print.output = FALSE)
```

```{r seurat-save}
saveRDS(pbmc, here("output/pbmc_clustered.Rds"))
```

```{r seurat-tree, fig.cap = "Clustering tree of a dataset of 2700 Peripheral Blood Mononuclear Cells (PBMCs) showing results from clustering using Seurat with resolutions parameters from zero to one. At a resolution of 0.1 we see the formation four main branches, one of which continues to split up to a resolution of 0.5. At higher resolutions we see only minor changes in the clusters formed. Seurat labels clusters according to their size with Cluster 0 being the largest."}
pp <- clustree(pbmc, node_size_range = c(8, 25), node_text_size = 6,
               edge_width = 3) +
    guides(size = guide_legend(title = "Cluster size", order = 1),
           colour = guide_legend(title = "Resolution",
                                 keywidth = 3,
                                 override.aes = list(size = 10),
                                 ncol = 2,
                                 order = 2),
           edge_colour = guide_edge_colourbar(title = "Sample count",
                                              barwidth = 3,
                                              draw.ulim = TRUE,
                                              draw.llim = TRUE,
                                              order = 3),
           edge_alpha = guide_legend(title = "In-proportion",
                                     keywidth = 5,
                                     order = 4),
           ncol = 2) +
    theme(legend.title = element_text(size = 20),
          legend.text = element_text(size = 15))

ggsave(here("figures/seurat_tree.png"), pp, width = 16, height = 12)
ggsave(here("figures/seurat_tree.pdf"), pp, width = 16, height = 10)

include_graphics(here("figures/seurat_tree.png"))
```

We see that four main branches quickly form in this tree at a resolution of just
0.1. One of these branches, starting with Cluster 3 at resolution 0.1 remains
unchanged while the branch starting with Cluster 2 splits only once at a
resolution of 0.4. Most of the branching occurs in the branch starting with
Cluster 1 which consistently has sub-branches split off to form new clusters as
the resolution increases. There are two regions of stability in the tree; at
resolution 0.5-0.6 and resolution 0.7-1.0 where the branch starting the Cluster
0 splits in two.

Known marker genes are commonly used to identify the cell types that sepecific
clusters correspond to. Overlaying gene expression information onto a clustering
tree provides an alternative view that can help to indicate when clusters
containing pure cell populations are formed. Figure \@ref(fig:seurat-genes)
shows the PBMC clustering tree overlaid with the expression of some known marker
genes.

```{r seurat-genes, fig.cap = "Clustering trees of the PBMC dataset coloured according to the expression of known markers. The node colours indicate the average of the log_2_ gene counts of samples in each cluster. CD19 (A) identifies B cells, CD14 (B) shows a population of monocytes, CD3D (C) is a marker of T cells and CCR7 (D) shows the split between memory and naive CD4 T cells."}

plots <- lapply(c("CD19", "CD14", "CD3D", "CCR7"), function(x) {
    clustree(pbmc, node_colour = x, node_colour_aggr = "mean",
             node_size_range = c(6, 20), node_text_size = 4,
             edge_width = 2.5) +
    scale_colour_viridis_c(option = "plasma", begin = 0.3) +
    guides(size = FALSE,
           colour = guide_colourbar(title = x,
                                    barwidth = 2,
                                    barheight = 30,
                                    order = 2),
           edge_colour = FALSE,
           edge_alpha = FALSE) +
    theme(legend.title = element_text(size = 20),
          legend.text = element_text(size = 15))    
})

legend <- get_legend(plots[[1]] +
                         guides(size = guide_legend(title = "Cluster size",
                                                    title.position = "top",
                                                    order = 1),
                                colour = FALSE,
                                edge_colour = guide_edge_colourbar(
                                    title = "Sample count",
                                    title.position = "top",
                                    barwidth = 12,
                                    draw.ulim = TRUE,
                                    draw.llim = TRUE,
                                    order = 3),
                                edge_alpha = guide_legend(
                                    title = "In-proportion",
                                    title.position = "top",
                                    keywidth = 4,
                                    order = 4)) +
                         theme(legend.position = "bottom"))
panel <- plot_grid(plotlist = plots,
                   labels = c("A - CD19", "B - CD14", "C - CD3D", "D - CCR7"),
                   label_size = 30, hjust = -0.1)
panel_legend <- plot_grid(panel, legend, ncol = 1, rel_heights = c(1, .1))

save_plot(here("figures/seurat_genes.png"), panel_legend, ncol = 2,
          base_height = 16, base_aspect_ratio = 0.8)
save_plot(here("figures/seurat_genes.pdf"), panel_legend, ncol = 2,
          base_height = 16, base_aspect_ratio = 0.8)

include_graphics(here("figures/seurat_genes.png"))
```

By adding this extra information we can quickly identify some of the cell types.
CD19 (Figure \@ref(fig:seurat-genes)A) is a marker of B cells and is clearly
expressed in the most distinct branch of the tree. CD14 (Figure
\@ref(fig:seurat-genes)B) is a marker of one type of monocytes which becomes
more expressed as we follow one of the central branches, allowing us to see
which resolution identifies a pure population of these cells. CD3D (Figure
\@ref(fig:seurat-genes)C) is a general marker of T cells and is expressed in two
separate branches, one which splits into low and high expression of CCR7 (Figure
\@ref(fig:seurat-genes)D), separating memory and naive CD4 T cells. By adding
expression of known genes to a clustering tree we can see if more populations
can be identified as resolution is increased and if clusters are split in
biologically sensible ways. For most of the Seurat tutorial a resolution of 0.6
is used but the authors note that by moving to a resolution of 0.8 a split can
be achieved between memory and naive CD4 T cells. This is a split that could be
anticipated by looking at the clustering tree.

# Discussion and conclusion

Clustering similar samples into groups is a useful technique in many fields but
often analysts are faced with the tricky problem of deciding which clustering
resolution to use. Traditional approaches to this problem typically consider
a single cluster or sample at a time and may rely on prior knowledge of sample
labels. Here we present clustering trees, an alternative visualisation that
shows the relationships between clusterings at multiple resolutions.

Clustering trees display how clusters are divided as resolution increases, which
cluster are clearly separate and distinct, which are related to each other and
how samples change groups as more clusters are produced. Although clustering
trees can appear similar to the trees produced from hierarchical clustering
there are several important difference. Hierarchical clustering considers the
relationships between individual samples and doesn't provide an obvious way to
form groups. In contrast clustering trees are independent of any particular
clustering method and show the relationships between distinct groups, any of
which could be used for further analysis.

To illustrate the uses of clustering trees we presented two examples, one using
the classical iris dataset and a second based on a complex scRNA-seq dataset.
Both examples demonstrate how a clustering tree can suggest the correct
resolution to use and how overlaying extra information can help to validate
those clusters. This is of particular use to scRNA-seq analysis as these
dataset are often large, noisy and contain an unknown number of cell types.

Even when the number of clusters to choose is not a problem clustering trees can
be a valuable tool. They provide a compact, information dense, visualisation
that can display summarised information across a range of clusters. By modifying
the appearance of cluster nodes based on attributes of the samples they
represent, clusterings can be evaluated and identities of clusters established.
In the future clustering trees could be adapted to be more flexible, such as
accommodating fuzzy clusterings, and may have applications in other fields.

# Methods

The clustree software package is built for the R statistical programming
language. It relies on the ggraph package (https://github.com/thomasp85/ggraph),
which is itself built on the ggplot2 [@Wickham2010-zq] and tidygraph packages
(https://github.com/thomasp85/tidygraph). Clustering trees are displayed using
the Reingold-Tilford tree layout [@Reingold1981-iy] or the Sugiyama layout 
[@Sugiyama1981-wu], both available as part of the igraph package
[@Csardi2006-ce].

The iris dataset is available as part of R. We clustered this dataset using the
kmeans function in the stats package with values of $k$ from one to five. Each
value of $k$ was clustered with a maximum of 100 iterations and with 10 random
starting positions. The clustered Iris dataset is available as part of the
clustree package.

The PBMC dataset was downloaded from the Seurat tutorial page
(http://satijalab.org/seurat/pbmc3k_tutorial.html) and this tutorial was
followed for most of the analysis. Briefly cells were filtered based on the
number of genes they express and the percentage of counts assigned to
mitochondrial genes. The data was then log-normalised and 1838 variable genes
identified. Potential confounding variables (number of unique molecular
identifiers and percentage mitochondrial expression) were regressed from the
dataset before performing principal component analysis on the identified
variable genes. The first 10 principal components were then used to build a
graph which was partitioned into clusters using Louvain modularity optimisation
[@Blondel2008-ym] with resolution parameters in the range zero to one, in steps
of 0.1.

# Declarations

## Ethics

Not applicable.

## Availability of data and materials

The clustree package is available from our GitHub repository
(https://github.com/Oshlack/clustree) and the code and datasets used for the
analysis in this paper are available from
https://github.com/Oshlack/clustree-paper.

## Competing interests

The authors declare no competing interests.

## Funding

Luke Zappia is supported by an Australian Government Research Training Program
(RTP) Scholarship. Alicia Oshlack is supported through a National Health and
Medical Research Council Career Development Fellowship APP1126157. MCRI is
supported by the Victorian Government's Operational Infrastructure Support
Program.

## Authors' contributions

## Acknowledgements

# Additional files

# References
