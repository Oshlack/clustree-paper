---
title: "Clustering trees, a visualisation for examining clusterings a increasing resolutions"
author:
- Luke Zappia (1, 2)
- Alicia Oshlack (1, 2)
date: "1 Bioinformatics, Murdoch Children's Research Institute; 2 School of Biosciences, University of Melbourne"
output:
    bookdown::word_document2:
        reference_docx: style/style.docx
        pandoc_args: [ "--csl", "style/nature.csl" ]
bibliography: style/references.bib
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(autodep        = TRUE,
                      cache          = TRUE,
                      cache.path     = "cache/",
                      cache.comments = TRUE,
                      echo           = FALSE,
                      error          = FALSE,
                      fig.path       = "figures/",
                      fig.width      = 10,
                      fig.height     = 8,
                      dev            = c('png', 'pdf'),
                      message        = FALSE,
                      warning        = FALSE)
```

```{r libaries, cache = FALSE}
library("clustree")
library("Seurat")
library("Matrix")

library("here")
library("knitr")
library("ggplot2")
library("cowplot")
```

```{r source}

```

```{r palette}

```

```{r load}

```

# Abstract

# Keywords

# Introduction

* Clustering used for many things
* Selecting number of clusters is hard
* Usually measure a single clustering
* Look at changes across resolutions

Clustering analysis is commonly used to group similar samples for a diverse
range of applications. Typically the goal of clustering is to form groups where
each sample is more similar to the other samples in the same group than to
samples in other groups. While other forms of clustering exist, such as fuzzy or
soft clustering where each sample is assigned to every cluster with some
probability or hierarchical clustering which forms a tree of samples, most
methods form hard clusters where each sample is assigned to a single group.
This goal can be achieved in a variety of ways, for example by considering the
distances between sample, areas of density across the dataset or relationships
to statistical distributions.

In many cases the number of groups that should be present in a dataset is not
known in advance and deciding the correct number of clusters to use is a
significant challenge. For some algorithms, such as $k$-means clustering, the
number of clusters must be explicitly provided while others have parameters
that, directly or indirectly, control the resolution of the clustering produced.
While there are methods and statistics (such as the elbow method or silhouette)
designed to help analysts decide which clustering resolution to use they
typically only consider a single set of samples or clusters at a time.

An alternative approach would be to consider clusterings at multiple resolutions
and look at how samples change groupings as the number of clusters increases.
This is the approach taken by the clustering tree visualisation we present here:
a dataset is clustered at multiple resolutions, the overlap between clusters at
adjacent resolutions is used to build edges and the resulting graph is
presented as a tree. This tree can be used to examine how clusters are
related to each other, which clusters are distinct and which are unstable. In
the following sections we describe how we construct such a tree and present
examples of trees built from a classical clustering dataset and a complex
single-cell RNA-sequencing (scRNA-seq) dataset. The kinds of figures shown here
can be produced in R using our publicly available clustree package.

# Building a clustering tree

* Cluster data at multiple resolutions
* Compare clusterings at adjacent resolutions
* Calculate statistics
* Build graph
* Visualise tree

To build a clustering tree we start with a set of clusterings allocating samples
to groups at different resolutions. These could be produced using any
hard-clustering algorithm that allows control of the number of clusters in
some way. For example this could be a set of samples clustered using $k$-means
with $k = 1, 2, 3$ as shown in Figure \@ref(fig:algorithm). We then sort these
clusterings so that they are ordered by increasing resolution (here that would
be $k$), then consider pairs of adjacent clusterings. Each cluster at a specific
resolution ($k$) is compared with each cluster in the next highest resolution
($k + 1$) and the overlap is computed as the number of samples that are assigned
to both. We next build a graph where each node is a cluster and each edge is an
overlap between clusters.

```{r algorithm, fig.cap = "Illustration of the steps required to build a clustering tree. First a dataset must be clustered at different resolutions. A graph in then constructed with nodes representing clusters and edges showing the overlap in samples between them. Statistics are then calculate, edges filtered and the final graph visualised.", cache = FALSE}
include_graphics(here("figures/algorithm.png"))
```

Many of the edges will be empty (no samples in Cluster A at resolution $k$ end
up in Cluster B at resolution $k + 1$). In some datasets there may also be edges
that contain few samples which are not informative and result in a cluttered
tree. This is easily handled by setting a threshold number of samples to remove
low-count edges. While this approach can be useful for removing edges from the
graph it is often more informative to instead emphasise more important edges.
In this case the count of samples is not the correct statistic to use as it
naturally favours edges at higher resolutions and those connecting larger
clusters. Instead we propose using the in proportion, the ratio between the
number of samples on the edge and the number of samples in the cluster it goes
towards. This measure shows the importance of the edge to the higher resolution
cluster, independently of the cluster size. We can apply a second threshold to
this value in order to remove less important edges.

We then visualise the final graph using a tree layout. This places the cluster
nodes in a series of layers where each layer is a different resolution and edges
show the transfer of samples through those resolutions. Edges are coloured
according to the number of samples they represent and the in proportion is used
to control the edge transparency, highlighting more important edges. By default
nodes are sized according the size of the cluster they represent, with their
colour indicating the resolution. The clustree package also includes options
for controlling the aesthetics of nodes based on the attributes of samples in
the clusters they represent.

# A simple example

* Iris dataset
* Clustered using k-means
* Features of tree...

To further illustrate how a clustering tree is built we will work through an
example using the classical iris dataset. This dataset contains measurements of
the sepal length, sepal width, petal length and petal width from 150 iris
flowers, 50 from each of three species: _Iris setosa_, _Iris versicolor_ and
_Iris virginica_. The iris dataset is commonly used as example for both
clustering and classification problems with the _Iris setosa_ samples being
significantly different to, and linearly separable from, the other samples. We
have clustered this dataset using $k$-means clustering with $k = 1,...,5$ and
produced the clustering tree shown in Figure \@ref(fig:iris-tree)A.

```{r iris-tree, fig.cap = "Clustering trees based on $k$-means clustering of the iris dataset. In A nodes are coloured according to the value of $k$ and sized according to the number of samples they represent. Edges are coloured according to the number of samples (from blue representing few to yellow reprenting many) and the transparency adjusted according to the in proportion, with stronger lines showing edges that are more important to the higher resolution cluster. B shows the same tree with the node colouring changed to show the mean petal length of the samples in each cluster."}
data("iris_clusts")

p1 <- clustree(iris_clusts, prefix = "K")
p2 <- clustree(iris_clusts, prefix = "K", node_colour = "Petal.Length",
               node_colour_aggr = mean) +
    scale_colour_viridis_c(option = "plasma", begin = 0.3)
panel <- plot_grid(p1, p2, labels = "AUTO")

save_plot(here("figures/iris_tree.png"), panel, ncol = 2, base_height = 8)
save_plot(here("figures/iris_tree.pdf"), panel, ncol = 2, base_height = 8)

include_graphics(here("figures/iris_tree.png"))
```

We see that there is one branch of the tree that is clearly distinct (presumably
representing _Iris setosa_), remaining unchanged regardless of the number of
clusters. On the other side we see the cluster at $k = 2$ cleanly split into two
clusters (presumably _Iris versicolor_ and _Iris virginica_) at $k = 3$ but as
we move to $k = 4$ and $k = 5$ we see clusters being formed from multiple
branches with more low proportion edges. This kind of pattern can be an
indication that the data has become over-clustered and we have begun to
introduce artificial groupings. In this case we know that $k = 3$ is the correct
choice but this is also the value that is suggested by this tree.

We can check our assumption that the distinct branch represents the _Iris
setosa_ samples and the other two clusters at $k = 3$ are _Iris versicolor_ and
_Iris virginica_ by overlaying some known information about the samples. In
Figure \@ref(fig:iris-tree)B we have coloured the nodes by the mean petal width
of the samples they contain. We can now see that clusters in the distinct branch
have the shortest petals, with Cluster 1 at $k = 3$ having an intermediate
length and Cluster 3 the longest petals. This feature is known to separate the
samples into the expected species with _Iris setosa_ have the shortest petals on
average, _Iris versicolor_ an intermediate length and _Iris virginica_ the
longest.

Although this is a very simple example it still highlights some of the benefits
of viewing a clustering tree. We get some indication of the correct resolution
to use by examining the edges and we can overlay known information to assess the
quality of the clustering. For example if we that all clusters had the same
mean petal length that we would be a good suggestion that the clustering has
not been successful as we know this is an important feature in this dataset that
should separate the species. We could potentially learn more by looking at which
samples follow low proportion edges or overlaying a series of features to try
and understand what particular clusters to split.

# Clustering trees for scRNA-seq data

* Clustering used for scRNA-seq data
* PBMC dataset
* Clustered using Seurat
* Features of tree...

```{r seurat-load}
pbmc_data <- Read10X(here("data"))
pbmc <- CreateSeuratObject(raw.data = pbmc_data, min.cells = 3, min.genes = 200,
                           project = "10X_PBMC")
```

```{r seurat-QC}
mito_genes <- grep(pattern = "^MT-", x = rownames(x = pbmc@data), value = TRUE)
percent_mito <- Matrix::colSums(pbmc@raw.data[mito_genes, ]) /
                Matrix::colSums(pbmc@raw.data)
pbmc <- AddMetaData(object = pbmc, metadata = percent_mito,
                    col.name = "percent_mito")
pbmc <- FilterCells(object = pbmc, subset.names = c("nGene", "percent_mito"),
                    low.thresholds = c(200, -Inf),
                    high.thresholds = c(2500, 0.05))
```

```{r seurat-norm}
pbmc <- NormalizeData(object = pbmc, normalization.method = "LogNormalize",
                      scale.factor = 10000, display.progress = FALSE)
pbmc <- FindVariableGenes(object = pbmc, mean.function = ExpMean,
                          dispersion.function = LogVMR, x.low.cutoff = 0.0125,
                          x.high.cutoff = 3, y.cutoff = 0.5, do.plot = FALSE,
                          display.progress = FALSE)
```

```{r seurat-scale}
pbmc <- ScaleData(object = pbmc, vars.to.regress = c("nUMI", "percent_mito"),
                  display.progress = FALSE)
```

```{r seurat-cluster}
pbmc <- RunPCA(object = pbmc, pc.genes = pbmc@var.genes, do.print = FALSE)
pbmc <- FindClusters(object = pbmc, reduction.type = "pca", dims.use = 1:10,
                     resolution = seq(0, 1, 0.1), print.output = FALSE)
```

```{r seurat-tree}
clustree(pbmc)
```

# Discussion and conclusion

* Picking number of clusters is hard
* Clustering trees let you see what happens as resolution increases

# Methods

# Declarations

## Ethics

Not applicable.

## Availability of data and materials

The clustree package is available from our GitHub repository
(https://github.com/Oshlack/clustree) and the code and datasets used for the
analysis in this paper are available from
https://github.com/Oshlack/clustree-paper.

## Competing interests

The authors declare no competing interests.

## Funding

Luke Zappia is supported by an Australian Government Research Training Program
(RTP) Scholarship. Alicia Oshlack is supported through a National Health and
Medical Research Council Career Development Fellowship APP1126157. MCRI is
supported by the Victorian Government's Operational Infrastructure Support
Program.

## Authors' contributions

## Acknowledgements

# Additional files

# References
